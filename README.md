# WEB-SCRAPING

# Web Scraping for Job Postings
This code scrapes data from a job portal to collect the following features for various roles in the data science field: 'Company', 'Role', 'Location', 'Date_Posted', 'Job_link', 'Employment_Type', and 'No_of_applicants'. The roles included in the web scraping process are 'Data Scientist', 'Data Engineer', 'Data Analyst', 'Data', and 'Machine Learning Engineer'.

# Libraries Used
BeautifulSoup: for parsing HTML content
Requests: for sending HTTP requests to the website to be scraped
Pandas: for storing the scraped data in a structured format

# Running the Code
To run the code, ensure that the above-mentioned libraries are installed. 
Then, simply run the web_scraping.py file and wait for the script to complete the scraping process. The data will be stored in a CSV file named job_postings.csv.

The collected data of the job postings can be used for various analyses such as identifying the trending job titles, locations with the highest demand for data professionals, and the most sought-after job skills. The data can also be used to gain insights into the hiring patterns of different companies in the data field. This information can be valuable for job seekers looking to improve their chances of landing a job in the data field.
